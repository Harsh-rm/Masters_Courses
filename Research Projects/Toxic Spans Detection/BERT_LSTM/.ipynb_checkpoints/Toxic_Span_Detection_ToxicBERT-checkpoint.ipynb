{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dschCt4G8bfB"
   },
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/harsh/anaconda3/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: packaging in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 51019,
     "status": "ok",
     "timestamp": 1613030042495,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "AD8Q7xVmA-pw",
    "outputId": "c7e167dc-a82c-4ea7-8233-38563be9c018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.5.0\n",
      "  Downloading tensorflow_gpu-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.4 MB 11 kB/s s eta 0:00:01     |█████████████████▊              | 251.9 MB 10.6 MB/s eta 0:00:20\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (0.37.1)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (0.2.0)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (0.4.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (3.19.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (1.6.3)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard~=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (2.10.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorflow-gpu==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (2.0.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/harsh/anaconda3/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.0) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu==2.5.0) (3.2.1)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=9b589158160585abb57355a657db34eff0ba567e6a55d8afa48baca266eae858\n",
      "  Stored in directory: /home/harsh/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: six, numpy, grpcio, absl-py, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, h5py, flatbuffers, tensorflow-gpu\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.2.0\n",
      "    Uninstalling absl-py-1.2.0:\n",
      "      Successfully uninstalled absl-py-1.2.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.0.1\n",
      "    Uninstalling termcolor-2.0.1:\n",
      "      Successfully uninstalled termcolor-2.0.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.10.0\n",
      "    Uninstalling tensorflow-estimator-2.10.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.6.0\n",
      "    Uninstalling h5py-3.6.0:\n",
      "      Successfully uninstalled h5py-3.6.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 22.9.24\n",
      "    Uninstalling flatbuffers-22.9.24:\n",
      "      Successfully uninstalled flatbuffers-22.9.24\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "tensorflow 2.10.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow 2.10.0 requires flatbuffers>=2.0, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.10.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "tensorflow 2.10.0 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n",
      "pydantic 1.10.2 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "bokeh 2.4.2 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed absl-py-0.15.0 flatbuffers-1.12 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 six-1.15.0 tensorflow-estimator-2.5.0 tensorflow-gpu-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu==2.5.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57961,
     "status": "ok",
     "timestamp": 1613030049440,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "O8TS_KwVA_IF",
    "outputId": "550cc6b3-0264-4696-9467-7b7189a2031b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/harsh/anaconda3/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/harsh/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsEbmTJsGBca"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3sRXbs_PWDAx"
   },
   "outputs": [],
   "source": [
    "# Maximum length of comment\n",
    "max_len = 128 \n",
    "# Dimension of embedding vector\n",
    "embedding_dim = 100 \n",
    "# Max feature\n",
    "max_feature = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2Lcz8BrSRC0L"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "data = pd.read_csv('/home/harsh/Downloads/tsd_train.csv')\n",
    "dev = pd.read_csv('/home/harsh/Downloads/tsd_trial.csv')\n",
    "test = pd.read_csv('/home/harsh/Downloads/tsd_test.csv')\n",
    "\n",
    "text_data = data['text'].values\n",
    "spans = data['spans'].apply(literal_eval)\n",
    "lbl = [1 if len(s) > 0 else 0 for s in spans]\n",
    "\n",
    "text_data_test = test['text'].values\n",
    "spans_test = test['spans'].apply(literal_eval)\n",
    "test_id = test.index\n",
    "lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n",
    "\n",
    "text_data_dev = dev['text'].values\n",
    "spans_dev = dev['spans'].apply(literal_eval)\n",
    "dev_id = dev.index\n",
    "lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/harsh/anaconda3/lib/python3.9/site-packages (3.4.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/harsh/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow 2.10.0 requires flatbuffers>=2.0, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.10.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "tensorflow 2.10.0 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n",
      "tensorflow-gpu 2.5.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
      "Successfully installed typing-extensions-4.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8O6SVdILRyAh"
   },
   "outputs": [],
   "source": [
    "# Token level \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "# import spacy\n",
    "\n",
    "tknzr2 = TweetTokenizer()\n",
    "\n",
    "def custom_tokenizer(text_data):\n",
    "    return tknzr2.tokenize(text_data)\n",
    "\n",
    "def retrieve_word_from_span(lst_span, text):\n",
    "    i = 0\n",
    "    token = []\n",
    "    a = 0\n",
    "\n",
    "    word = []\n",
    "\n",
    "    while (i < (len(lst_span) - 1)):\n",
    "        if (lst_span[i] != (lst_span[i+1]-1)):\n",
    "            token.append(lst_span[a:(i+1)])\n",
    "            a = i + 1\n",
    "        elif i == (len(lst_span) - 2):\n",
    "            token.append(lst_span[a:i+2])\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    for t in token:\n",
    "        word.append(text[t[0]:(t[len(t)-1])+1])\n",
    "\n",
    "    return word\n",
    "\n",
    "def span_retrived(text_data, spans):\n",
    "    token_labels = []\n",
    "\n",
    "    for i in range(0, len(text_data)):\n",
    "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
    "    \n",
    "    return token_labels\n",
    "\n",
    "def span_convert(text_data, spans):\n",
    "    MAX_LEN = 0\n",
    "    token_labels = []\n",
    "\n",
    "    for i in range(0, len(text_data)):\n",
    "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
    "\n",
    "    lst_seq = []\n",
    "    for i in range(0, len(text_data)):\n",
    "        # token = tknzr.tokenize(text_data[i])\n",
    "        token = custom_tokenizer(text_data[i])\n",
    "        if len(token) > MAX_LEN:\n",
    "            MAX_LEN = len(token)\n",
    "            \n",
    "        seq = np.zeros(len(token), dtype=int)\n",
    "        for j in range(0, len(token)):\n",
    "            for t in token_labels[i]:\n",
    "                # if token[j] in tknzr.tokenize(t):\n",
    "                if token[j] in custom_tokenizer(t):\n",
    "                    seq[j] = 1\n",
    "        lst_seq.append(seq)     \n",
    "\n",
    "    return (token_labels, lst_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jI5OfwZYPaMO"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# convert data\n",
    "data['token'], data['seq'] = span_convert(text_data, spans)\n",
    "dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n",
    "test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n",
    "\n",
    "train = deepcopy(data)\n",
    "data = pd.concat([data, dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaHOdIeNKBqd"
   },
   "source": [
    "# Evaluation metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "as0mrBcck7eR"
   },
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
    "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
    "    :param predictions: a list of predicted offsets\n",
    "    :param gold: a list of offsets serving as the ground truth\n",
    "    :return: a score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1. if len(predictions) == 0 else 0.\n",
    "    if len(predictions) == 0:\n",
    "        return 0.\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)\n",
    "\n",
    "\n",
    "def evaluate(pred, gold):\n",
    "    \"\"\"\n",
    "    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n",
    "    :param pred: file with predictions\n",
    "    :param gold: file with ground truth\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # # read the predictions\n",
    "    # pred_lines = pred.readlines()\n",
    "    # # read the ground truth\n",
    "    # gold_lines = gold.readlines()\n",
    "\n",
    "    pred_lines = pred\n",
    "    gold_lines = gold\n",
    "\n",
    "    # only when the same number of lines exists\n",
    "    if (len(pred_lines) == len(gold_lines)):\n",
    "        data_dic = {}\n",
    "        for n, line in enumerate(gold_lines):\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n",
    "            else:\n",
    "                raise ValueError('Format problem for gold line %d.', n)\n",
    "\n",
    "        for n, line in enumerate(pred_lines):\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                if int(parts[0]) in data_dic:\n",
    "                    try:\n",
    "                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[int(parts[0])].append([])\n",
    "                else:\n",
    "                    raise ValueError('Invalid text id for pred line %d.', n)\n",
    "            else:\n",
    "                raise ValueError('Format problem for pred line %d.', n)\n",
    "\n",
    "        # lists storing gold and prediction scores\n",
    "        scores = []\n",
    "        for id in data_dic:\n",
    "            if len(data_dic[id]) == 2:\n",
    "                gold_spans = data_dic[id][0]\n",
    "                pred_spans = data_dic[id][1]\n",
    "                scores.append(f1(pred_spans, gold_spans))\n",
    "            else:\n",
    "                sys.exit('Repeated id in test data.')\n",
    "\n",
    "        return (np.mean(scores), sem(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2WZCOctatQs"
   },
   "source": [
    "# Data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5537,
     "status": "ok",
     "timestamp": 1613034436786,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "05MJABVLa97Y",
    "outputId": "00eeac09-fc55-445d-873e-944768e60fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "# Number of training data\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4666,
     "status": "ok",
     "timestamp": 1613034436789,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "T3jc4SouVmH4",
    "outputId": "89f2a98d-7759-4e67-a559-852da86467cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Number of test data\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "executionInfo": {
     "elapsed": 2930,
     "status": "ok",
     "timestamp": 1613034436789,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "M9gTn4FLV4rL",
    "outputId": "66317e96-77ee-4ea5-91b6-9f4e522ec65e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "      <td>[ABNORMAL, sexist rubbish]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[81, 82, 83, 84, 85, 86]</td>\n",
       "      <td>\"Watch people die from taking away their healt...</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>tens years ago i contacted the PDR and suggest...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>The parallels between the ANC and the Sicilian...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>Intel Community: ‘How can we work for a Presid...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n",
       "      <td>hey loser change your name to something more a...</td>\n",
       "      <td>[loser, ignorant, loser]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>[23, 24, 25, 26, 27]</td>\n",
       "      <td>And you are a complete moron who obviously doe...</td>\n",
       "      <td>[moron]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n",
       "      <td>Such vitriol from the left.  Who would have th...</td>\n",
       "      <td>[hypocrites]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>[]</td>\n",
       "      <td>It is now time for most of you to expand your ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>[828, 829, 830, 831]</td>\n",
       "      <td>Why does this author think she can demand, or ...</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...   \n",
       "1                              [81, 82, 83, 84, 85, 86]   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...   \n",
       "1996                               [23, 24, 25, 26, 27]   \n",
       "1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...   \n",
       "1998                                                 []   \n",
       "1999                               [828, 829, 830, 831]   \n",
       "\n",
       "                                                   text  \\\n",
       "0     That's right. They are not normal. And I am st...   \n",
       "1     \"Watch people die from taking away their healt...   \n",
       "2     tens years ago i contacted the PDR and suggest...   \n",
       "3     The parallels between the ANC and the Sicilian...   \n",
       "4     Intel Community: ‘How can we work for a Presid...   \n",
       "...                                                 ...   \n",
       "1995  hey loser change your name to something more a...   \n",
       "1996  And you are a complete moron who obviously doe...   \n",
       "1997  Such vitriol from the left.  Who would have th...   \n",
       "1998  It is now time for most of you to expand your ...   \n",
       "1999  Why does this author think she can demand, or ...   \n",
       "\n",
       "                           token  \\\n",
       "0     [ABNORMAL, sexist rubbish]   \n",
       "1                       [stupid]   \n",
       "2                             []   \n",
       "3                             []   \n",
       "4                             []   \n",
       "...                          ...   \n",
       "1995    [loser, ignorant, loser]   \n",
       "1996                     [moron]   \n",
       "1997                [hypocrites]   \n",
       "1998                          []   \n",
       "1999                      [dumb]   \n",
       "\n",
       "                                                    seq  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1995  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "1996  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1998         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1613034447824,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "tu3EjmPXbB8A",
    "outputId": "5d4890e3-fb1f-4c29-8dc4-08f32dc16cc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>[49, 50, 51, 52, 53, 54]</td>\n",
       "      <td>Ah, so sad. It certainly does seem to bother t...</td>\n",
       "      <td>[trolls]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n",
       "      <td>Yes, let's end the insanity!  No more trans pe...</td>\n",
       "      <td>[No more trans people! No more boys kissing bo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>Such garbage logic by republicans which will b...</td>\n",
       "      <td>[Such garbage logic by republicans]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...</td>\n",
       "      <td>aa the usual left wing attack of stupidity is ...</td>\n",
       "      <td>[stupidity, crying that wont stop]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7923</th>\n",
       "      <td>[37, 38, 39, 40, 41]</td>\n",
       "      <td>The G&amp;M doesn't need to repeat EVERY idiot thi...</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>[177, 178, 179, 180, 181, 182, 183, 184, 185]</td>\n",
       "      <td>All this reality tv suspense, all this loud no...</td>\n",
       "      <td>[stupidity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>[15, 16, 17, 18, 19, 20]</td>\n",
       "      <td>Can't get over idiots like yourself who probab...</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>[31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...</td>\n",
       "      <td>I only use the word haole when stupidity and a...</td>\n",
       "      <td>[stupidity, arrogance]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>[132, 133, 134, 135]</td>\n",
       "      <td>Pity Vegter doesn't apply the same rigour he n...</td>\n",
       "      <td>[dick]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>[54, 55, 56, 57]</td>\n",
       "      <td>Anyone who voted for Hillary is blind, gullibl...</td>\n",
       "      <td>[fool]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7929</th>\n",
       "      <td>[26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...</td>\n",
       "      <td>No accountability on both sides. No clue on wh...</td>\n",
       "      <td>[sides, idiots]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>[320, 321, 322, 323, 324, 350, 351, 352, 389, ...</td>\n",
       "      <td>Yes, it is OK. I haven't seen any rioting and ...</td>\n",
       "      <td>[dirty, gay, racist, morons]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>[44, 45, 46, 47, 48, 49, 50, 51]</td>\n",
       "      <td>In other word, blah, blah, blah, blah. It's bu...</td>\n",
       "      <td>[bullshit]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>[218, 219, 220, 221, 222, 223]</td>\n",
       "      <td>Driving like that is why people are going to s...</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>[4, 5, 6, 7, 8, 35, 36, 37, 38]</td>\n",
       "      <td>The idiot in chief can't speak for sh!t 😜👎</td>\n",
       "      <td>[idiot, sh!t]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>[8, 9, 10, 11]</td>\n",
       "      <td>Another fool pipes in.</td>\n",
       "      <td>[fool]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n",
       "      <td>So if a restaurant owner puts up a sign saying...</td>\n",
       "      <td>[No Blacks Allowed]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>Any faith that can't stand up to logic and rea...</td>\n",
       "      <td>[Any faith that can't stand up to logic and re...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>This idiotic. Use the surplus to pay down the ...</td>\n",
       "      <td>[idiotic]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7938</th>\n",
       "      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n",
       "      <td>Who is this \"we\" of which you speak? Are you r...</td>\n",
       "      <td>[penis, women's bodies]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "7919                           [49, 50, 51, 52, 53, 54]   \n",
       "7920  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...   \n",
       "7921  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "7922  [33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...   \n",
       "7923                               [37, 38, 39, 40, 41]   \n",
       "7924      [177, 178, 179, 180, 181, 182, 183, 184, 185]   \n",
       "7925                           [15, 16, 17, 18, 19, 20]   \n",
       "7926  [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...   \n",
       "7927                               [132, 133, 134, 135]   \n",
       "7928                                   [54, 55, 56, 57]   \n",
       "7929  [26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...   \n",
       "7930  [320, 321, 322, 323, 324, 350, 351, 352, 389, ...   \n",
       "7931                   [44, 45, 46, 47, 48, 49, 50, 51]   \n",
       "7932                     [218, 219, 220, 221, 222, 223]   \n",
       "7933                    [4, 5, 6, 7, 8, 35, 36, 37, 38]   \n",
       "7934                                     [8, 9, 10, 11]   \n",
       "7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...   \n",
       "7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "7937                            [5, 6, 7, 8, 9, 10, 11]   \n",
       "7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...   \n",
       "\n",
       "                                                   text  \\\n",
       "7919  Ah, so sad. It certainly does seem to bother t...   \n",
       "7920  Yes, let's end the insanity!  No more trans pe...   \n",
       "7921  Such garbage logic by republicans which will b...   \n",
       "7922  aa the usual left wing attack of stupidity is ...   \n",
       "7923  The G&M doesn't need to repeat EVERY idiot thi...   \n",
       "7924  All this reality tv suspense, all this loud no...   \n",
       "7925  Can't get over idiots like yourself who probab...   \n",
       "7926  I only use the word haole when stupidity and a...   \n",
       "7927  Pity Vegter doesn't apply the same rigour he n...   \n",
       "7928  Anyone who voted for Hillary is blind, gullibl...   \n",
       "7929  No accountability on both sides. No clue on wh...   \n",
       "7930  Yes, it is OK. I haven't seen any rioting and ...   \n",
       "7931  In other word, blah, blah, blah, blah. It's bu...   \n",
       "7932  Driving like that is why people are going to s...   \n",
       "7933         The idiot in chief can't speak for sh!t 😜👎   \n",
       "7934                             Another fool pipes in.   \n",
       "7935  So if a restaurant owner puts up a sign saying...   \n",
       "7936  Any faith that can't stand up to logic and rea...   \n",
       "7937  This idiotic. Use the surplus to pay down the ...   \n",
       "7938  Who is this \"we\" of which you speak? Are you r...   \n",
       "\n",
       "                                                  token  \\\n",
       "7919                                           [trolls]   \n",
       "7920  [No more trans people! No more boys kissing bo...   \n",
       "7921                [Such garbage logic by republicans]   \n",
       "7922                 [stupidity, crying that wont stop]   \n",
       "7923                                            [idiot]   \n",
       "7924                                        [stupidity]   \n",
       "7925                                           [idiots]   \n",
       "7926                             [stupidity, arrogance]   \n",
       "7927                                             [dick]   \n",
       "7928                                             [fool]   \n",
       "7929                                    [sides, idiots]   \n",
       "7930                       [dirty, gay, racist, morons]   \n",
       "7931                                         [bullshit]   \n",
       "7932                                           [idiots]   \n",
       "7933                                      [idiot, sh!t]   \n",
       "7934                                             [fool]   \n",
       "7935                                [No Blacks Allowed]   \n",
       "7936  [Any faith that can't stand up to logic and re...   \n",
       "7937                                          [idiotic]   \n",
       "7938                            [penis, women's bodies]   \n",
       "\n",
       "                                                    seq  \n",
       "7919         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "7920  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7921   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7922  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7923  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "7924  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7925  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7926  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "7927  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7928            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "7929  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7930  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7931  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "7932  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7933               [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  \n",
       "7934                                    [0, 1, 0, 0, 0]  \n",
       "7935  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...  \n",
       "7936         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]  \n",
       "7937  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7938  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show example of training data\n",
    "train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "viHoxDqEauyj"
   },
   "outputs": [],
   "source": [
    "# counting word in spans for train \n",
    "len_span = train['token'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sw7qNdR-VppI"
   },
   "outputs": [],
   "source": [
    "# counting word in spans for test \n",
    "len_span_test = test['token'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1613034465286,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "ZrGstC0whNKs",
    "outputId": "84f32782-36eb-4a21-ef8a-a26a080f8d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0.676534\n",
       "2     0.200655\n",
       "0     0.061091\n",
       "3     0.043330\n",
       "4     0.011588\n",
       "5     0.003401\n",
       "6     0.001512\n",
       "7     0.001008\n",
       "8     0.000504\n",
       "25    0.000126\n",
       "9     0.000126\n",
       "11    0.000126\n",
       "Name: token, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic spans by number of word in span for train \n",
    "len_span.value_counts(normalize=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1613034459712,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "jUR6XwF2VuEz",
    "outputId": "4a01a550-5700-4d4e-bca8-b91d7d65dea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.7035\n",
       "0    0.1970\n",
       "2    0.0860\n",
       "3    0.0080\n",
       "4    0.0040\n",
       "6    0.0010\n",
       "7    0.0005\n",
       "Name: token, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic spans by number of word in span for test \n",
    "len_span_test.value_counts(normalize=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1613034461050,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "gPJWuIpegKo1",
    "outputId": "6d3b9b0f-1523-42bc-eabc-09d6bf1d4c62"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution histogram plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(len_span, density=True, edgecolor='k', rwidth=0.8)  # density=False would make counts\n",
    "\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Number of span');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq3YWgcPFkB1"
   },
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19480,
     "status": "ok",
     "timestamp": 1613030249939,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "s4txBO5zya-5",
    "outputId": "a3703690-324f-4d99-c80a-0b4171c205b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "# Read embedding\n",
    "word_dict = []\n",
    "embeddings_index = {}\n",
    "f = open('/home/harsh/Downloads/glove.twitter.27B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] \n",
    "    word_dict.append(word)\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "n8bsiH8BSGjC"
   },
   "outputs": [],
   "source": [
    "words = word_dict\n",
    "num_words = len(words)\n",
    "\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "idx2word = {i: w for w, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3OBCNzNnas7p"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100) into shape (25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m embedding_vector \u001b[38;5;241m=\u001b[39m embeddings_index\u001b[38;5;241m.\u001b[39mget(word)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# we found the word - add that words vector to the matrix\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     embedding_matrix[i] \u001b[38;5;241m=\u001b[39m embedding_vector\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# doesn't exist, assign a random vector\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     embedding_matrix[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(embedding_dim)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100) into shape (25)"
     ]
    }
   ],
   "source": [
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_to_index.items():\n",
    "    if i > max_feature:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "exfoi9XHba3G"
   },
   "outputs": [],
   "source": [
    " # mapping for token cases\n",
    "case2Idx = {'1': 1, '0': 0}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmJ-NOODFb0Y"
   },
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZpsRfA9QF9mL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['seq']\n",
    "X = data['text']\n",
    "\n",
    "y_test = test['seq']\n",
    "X_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bsS9aVEVcJQO"
   },
   "outputs": [],
   "source": [
    "#train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19143,
     "status": "ok",
     "timestamp": 1613030250911,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "Z8GGEAurVu_c",
    "outputId": "17065691-6cc6-496a-d90e-d8b860869a20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/harsh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/harsh/nltk_data...\n",
      "2022-12-20 17:48:20.160155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 17:48:20.160173: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-20 17:48:21.277867: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/api/keras/optimizers\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Another metric with the same name already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Constant\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/__init__.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/models/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/functional.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer \u001b[38;5;28;01mas\u001b[39;00m input_layer_module\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast_variable\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale_optimizer\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_serialization\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/mixed_precision/loss_scale_optimizer.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m smart_cond\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_scale \u001b[38;5;28;01mas\u001b[39;00m keras_loss_scale_module\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/optimizers/__init__.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Imports needed for deserialization.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adadelta \u001b[38;5;28;01mas\u001b[39;00m adadelta_legacy\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adagrad \u001b[38;5;28;01mas\u001b[39;00m adagrad_legacy\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adam \u001b[38;5;28;01mas\u001b[39;00m adam_legacy\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/optimizers/legacy/adadelta.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Legacy Adadelta optimizer implementation.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adadelta\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adadelta.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v2\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m---> 38\u001b[0m keras_optimizers_gauge \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolGauge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tensorflow/api/keras/optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras optimizer usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m _DEFAULT_VALID_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[1;32m     43\u001b[0m     [\n\u001b[1;32m     44\u001b[0m         tf\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     ]\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deduplicate_indexed_slices\u001b[39m(values, indices):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/monitoring.py:352\u001b[0m, in \u001b[0;36mBoolGauge.__init__\u001b[0;34m(self, name, description, *labels)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, description, \u001b[38;5;241m*\u001b[39mlabels):\n\u001b[1;32m    345\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a new BoolGauge.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    *labels: The label list of the new metric.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBoolGauge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBoolGauge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_bool_gauge_methods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/monitoring.py:127\u001b[0m, in \u001b[0;36mMetric.__init__\u001b[0;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods):\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot create \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m metric with label >= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_name, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_methods)))\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_label_length\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "tknzr2 = TweetTokenizer()\n",
    "\n",
    "def custom_tokenizer(text_data):\n",
    "    text_data = text_data.lower()\n",
    "    return tknzr2.tokenize(text_data)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    for w in word_list:\n",
    "        w = lemma.lemmatize(w)\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in word_list:\n",
    "        new_text = new_text + \" \" + w\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def encoding(X, y, isTest = True):\n",
    "    sentences = []\n",
    "    \n",
    "    for t in X:\n",
    "        sentences.append(custom_tokenizer(t))\n",
    "\n",
    "    X = []\n",
    "    for s in sentences:\n",
    "        sent = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                w = w.lower()\n",
    "                sent.append(word_to_index[w])\n",
    "            except:\n",
    "                sent.append(word_to_index[\"UNK\"])\n",
    "        X.append(sent)\n",
    "           \n",
    "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
    "\n",
    "    if isTest:\n",
    "        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n",
    "        y = to_categorical(y, num_classes=2)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return (X,y)\n",
    "\n",
    "\n",
    "def decoding(text_data, encoding_text, prediction):\n",
    "    test = [[idx2word[i] for i in row] for row in encoding_text]\n",
    "\n",
    "    lst_token = []\n",
    "\n",
    "    for t in range(0, len(test)):\n",
    "        yy_pred = []\n",
    "        for i in range(0, len(test[t])):\n",
    "            if prediction[t][i] == 1:\n",
    "                yy_pred.append(test[t][i])\n",
    "        lst_token.append(yy_pred)\n",
    "\n",
    "    lis_idx = []\n",
    "    for i in range(0, len(text_data)):\n",
    "        idx = []\n",
    "        for t in lst_token[i]:\n",
    "            index = text_data[i].find(t)\n",
    "            idx.append(index)\n",
    "            for j in range(1, len(t)):\n",
    "                index = index + 1\n",
    "                idx.append(index)\n",
    "        lis_idx.append(idx)\n",
    "\n",
    "    return lis_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ajE_t3U-PiF"
   },
   "outputs": [],
   "source": [
    "X1, y1 = encoding(X_train, y_train)\n",
    "X2, y2 = encoding(X_dev, y_dev)\n",
    "X3, y3 = encoding(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21310,
     "status": "ok",
     "timestamp": 1613030253088,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "ozUGBMYyIAAu",
    "outputId": "7912ee18-23c8-469a-c207-492183eb67c8"
   },
   "outputs": [],
   "source": [
    "# Illustrating the data transforming \n",
    "x_t, y_t = encoding(X, y)\n",
    "\n",
    "print(custom_tokenizer(X[7926]))\n",
    "print(X[7926])\n",
    "print(x_t[7926])\n",
    "\n",
    "print(y[7926])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onI2IhquAwPi"
   },
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0wNXoS4Ax1J"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('unitary/toxic-bert')\n",
    "\n",
    "import torch\n",
    "\n",
    "class BuildDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "X_train_classify = text_data\n",
    "y_train_classify = lbl\n",
    "\n",
    "X_test_classify = text_data_test\n",
    "y_test_classify = lbl_test\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(X_train_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n",
    "test_encodings = tokenizer(X_test_classify.tolist(), truncation=True, padding=True, max_length=max_len)\n",
    "\n",
    "# train = pd.DataFrame({'text': train_encodings, 'labels': y_train_classify})\n",
    "# # dev = pd.DataFrame({'text': dev_X, 'labels': dev_y})\n",
    "# test = pd.DataFrame({'text': test_encodings, 'labels': y_test_classify})\n",
    "\n",
    "# train = pd.concat([train, test])\n",
    "\n",
    "train_dataset = BuildDataset(train_encodings, y_train_classify)\n",
    "test_dataset = BuildDataset(test_encodings, y_test_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hdUD-pNU6ua"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='drive/MyDrive/CODE/SemVal/results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model('drive/MyDrive/CODE/SemVal/model/transformers2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yG2B766GfFXX"
   },
   "outputs": [],
   "source": [
    "y_pred_classify = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRpJg0QYkCbf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "y_pred = y_pred_classify.label_ids\n",
    "y_true = y_test_classify\n",
    "\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "print(cf)\n",
    "\n",
    "evaluation = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "print(\"F1 - micro: \" + str(evaluation))\n",
    "\n",
    "evaluation = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"F1 - macro: \" + str(evaluation))\n",
    "\n",
    "evaluation = recall_score(y_true, y_pred)\n",
    "print(\"Recall: \" + str(evaluation))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP04rFC/qDtO0xNyms7MoeZ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1R-YJ3AXhSSJbaeV6IU1GsuWjRnpY4yd3",
   "name": "Toxic_Span_Detection_ToxicBERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
