{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dschCt4G8bfB"
   },
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/harsh/anaconda3/lib/python3.9/site-packages (22.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53280,
     "status": "ok",
     "timestamp": 1613028816710,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "Nq9Eehn8mZ01",
    "outputId": "0d4f420f-ede1-4c2d-a43a-968be612e584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3353,
     "status": "ok",
     "timestamp": 1613028860742,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "swOtuHtAmYkE",
    "outputId": "9728dbc7-f1e9-4817-a0df-654b36f471f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (2.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: h5py in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (3.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (6.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (1.19.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6879,
     "status": "ok",
     "timestamp": 1613028865951,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "OfQpKTQNnMJt",
    "outputId": "cb7402ae-f64d-4be1-fd3e-409f290a6ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-qzn91aio\n",
      "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-qzn91aio\n",
      "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
      "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: keras in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras-contrib==2.0.8) (2.2.4)\n",
      "Requirement already satisfied: h5py in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.7.3)\n",
      "Requirement already satisfied: pyyaml in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-4jp19k4x\n",
      "  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-4jp19k4x\n",
      "  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
      "  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: keras in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras-contrib==2.0.8) (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (6.0)\n",
      "Requirement already satisfied: h5py in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/harsh/anaconda3/lib/python3.9/site-packages (from keras->keras-contrib==2.0.8) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsEbmTJsGBca"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3sRXbs_PWDAx"
   },
   "outputs": [],
   "source": [
    "# Maximum length of comment\n",
    "max_len = 128 \n",
    "# Dimension of embedding vector\n",
    "embedding_dim = 100 \n",
    "# Max feature\n",
    "max_feature = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2Lcz8BrSRC0L"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "data = pd.read_csv('/home/harsh/Downloads/tsd_train.csv')\n",
    "dev = pd.read_csv('/home/harsh/Downloads/tsd_trial.csv')\n",
    "test = pd.read_csv('/home/harsh/Downloads/tsd_test.csv')\n",
    "\n",
    "text_data = data['text'].values\n",
    "spans = data['spans'].apply(literal_eval)\n",
    "lbl = [1 if len(s) > 0 else 0 for s in spans]\n",
    "\n",
    "text_data_test = test['text'].values\n",
    "spans_test = test['spans'].apply(literal_eval)\n",
    "test_id = test.index\n",
    "lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n",
    "\n",
    "text_data_dev = dev['text'].values\n",
    "spans_dev = dev['spans'].apply(literal_eval)\n",
    "dev_id = dev.index\n",
    "lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/harsh/anaconda3/lib/python3.9/site-packages (22.3.1)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.9/site-packages (65.6.3)\n",
      "Requirement already satisfied: wheel in /home/harsh/anaconda3/lib/python3.9/site-packages (0.38.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/harsh/anaconda3/lib/python3.9/site-packages (3.4.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/harsh/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy[cuda113] in /home/harsh/anaconda3/lib/python3.9/site-packages (3.4.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (1.19.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (2.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (21.3)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (65.6.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (8.1.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (2.27.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (3.0.10)\n",
      "Requirement already satisfied: cupy-cuda113<12.0.0,>=5.0.0b4 in /home/harsh/anaconda3/lib/python3.9/site-packages (from spacy[cuda113]) (10.6.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from cupy-cuda113<12.0.0,>=5.0.0b4->spacy[cuda113]) (0.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/harsh/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy[cuda113]) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy[cuda113]) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda113]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda113]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda113]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/harsh/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda113]) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy[cuda113]) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/harsh/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy[cuda113]) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/harsh/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy[cuda113]) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/harsh/anaconda3/lib/python3.9/site-packages (from jinja2->spacy[cuda113]) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy[cuda113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8O6SVdILRyAh"
   },
   "outputs": [],
   "source": [
    "# Token level \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "# import spacy\n",
    "\n",
    "tknzr2 = TweetTokenizer()\n",
    "\n",
    "def custom_tokenizer(text_data):\n",
    "    return tknzr2.tokenize(text_data)\n",
    "\n",
    "def retrieve_word_from_span(lst_span, text):\n",
    "    i = 0\n",
    "    token = []\n",
    "    a = 0\n",
    "\n",
    "    word = []\n",
    "\n",
    "    while (i < (len(lst_span) - 1)):\n",
    "        if (lst_span[i] != (lst_span[i+1]-1)):\n",
    "            token.append(lst_span[a:(i+1)])\n",
    "            a = i + 1\n",
    "        elif i == (len(lst_span) - 2):\n",
    "            token.append(lst_span[a:i+2])\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    for t in token:\n",
    "        word.append(text[t[0]:(t[len(t)-1])+1])\n",
    "\n",
    "    return word\n",
    "\n",
    "def span_retrived(text_data, spans):\n",
    "    token_labels = []\n",
    "\n",
    "    for i in range(0, len(text_data)):\n",
    "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
    "    \n",
    "    return token_labels\n",
    "\n",
    "def span_convert(text_data, spans):\n",
    "    MAX_LEN = 0\n",
    "    token_labels = []\n",
    "\n",
    "    for i in range(0, len(text_data)):\n",
    "        token_labels.append(retrieve_word_from_span(spans[i], text_data[i]))\n",
    "\n",
    "    lst_seq = []\n",
    "    for i in range(0, len(text_data)):\n",
    "        # token = tknzr.tokenize(text_data[i])\n",
    "        token = custom_tokenizer(text_data[i])\n",
    "        if len(token) > MAX_LEN:\n",
    "            MAX_LEN = len(token)\n",
    "            \n",
    "        seq = np.zeros(len(token), dtype=int)\n",
    "        for j in range(0, len(token)):\n",
    "            for t in token_labels[i]:\n",
    "                # if token[j] in tknzr.tokenize(t):\n",
    "                if token[j] in custom_tokenizer(t):\n",
    "                    seq[j] = 1\n",
    "        lst_seq.append(seq)     \n",
    "\n",
    "    return (token_labels, lst_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jI5OfwZYPaMO"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# convert data\n",
    "data['token'], data['seq'] = span_convert(text_data, spans)\n",
    "dev['token'], dev['seq'] = span_convert(text_data_dev, spans_dev)\n",
    "test['token'], test['seq'] = span_convert(text_data_test, spans_test)\n",
    "\n",
    "train = deepcopy(data)\n",
    "data = pd.concat([data, dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaHOdIeNKBqd"
   },
   "source": [
    "# Evaluation metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "as0mrBcck7eR"
   },
   "outputs": [],
   "source": [
    "# Evaluation metric\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
    "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
    "    :param predictions: a list of predicted offsets\n",
    "    :param gold: a list of offsets serving as the ground truth\n",
    "    :return: a score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1. if len(predictions) == 0 else 0.\n",
    "    if len(predictions) == 0:\n",
    "        return 0.\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)\n",
    "\n",
    "\n",
    "def evaluate(pred, gold):\n",
    "    \"\"\"\n",
    "    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n",
    "    :param pred: file with predictions\n",
    "    :param gold: file with ground truth\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # # read the predictions\n",
    "    # pred_lines = pred.readlines()\n",
    "    # # read the ground truth\n",
    "    # gold_lines = gold.readlines()\n",
    "\n",
    "    pred_lines = pred\n",
    "    gold_lines = gold\n",
    "\n",
    "    # only when the same number of lines exists\n",
    "    if (len(pred_lines) == len(gold_lines)):\n",
    "        data_dic = {}\n",
    "        for n, line in enumerate(gold_lines):\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n",
    "            else:\n",
    "                raise ValueError('Format problem for gold line %d.', n)\n",
    "\n",
    "        for n, line in enumerate(pred_lines):\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                if int(parts[0]) in data_dic:\n",
    "                    try:\n",
    "                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[int(parts[0])].append([])\n",
    "                else:\n",
    "                    raise ValueError('Invalid text id for pred line %d.', n)\n",
    "            else:\n",
    "                raise ValueError('Format problem for pred line %d.', n)\n",
    "\n",
    "        # lists storing gold and prediction scores\n",
    "        scores = []\n",
    "        for id in data_dic:\n",
    "            if len(data_dic[id]) == 2:\n",
    "                gold_spans = data_dic[id][0]\n",
    "                pred_spans = data_dic[id][1]\n",
    "                scores.append(f1(pred_spans, gold_spans))\n",
    "            else:\n",
    "                sys.exit('Repeated id in test data.')\n",
    "\n",
    "        return (np.mean(scores), sem(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2WZCOctatQs"
   },
   "source": [
    "# Data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5537,
     "status": "ok",
     "timestamp": 1613034436786,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "05MJABVLa97Y",
    "outputId": "00eeac09-fc55-445d-873e-944768e60fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "# Number of training data\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4666,
     "status": "ok",
     "timestamp": 1613034436789,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "T3jc4SouVmH4",
    "outputId": "89f2a98d-7759-4e67-a559-852da86467cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Number of test data\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "executionInfo": {
     "elapsed": 2930,
     "status": "ok",
     "timestamp": 1613034436789,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "M9gTn4FLV4rL",
    "outputId": "66317e96-77ee-4ea5-91b6-9f4e522ec65e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...</td>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "      <td>[ABNORMAL, sexist rubbish]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[81, 82, 83, 84, 85, 86]</td>\n",
       "      <td>\"Watch people die from taking away their healt...</td>\n",
       "      <td>[stupid]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>tens years ago i contacted the PDR and suggest...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>The parallels between the ANC and the Sicilian...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>Intel Community: ‘How can we work for a Presid...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n",
       "      <td>hey loser change your name to something more a...</td>\n",
       "      <td>[loser, ignorant, loser]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>[23, 24, 25, 26, 27]</td>\n",
       "      <td>And you are a complete moron who obviously doe...</td>\n",
       "      <td>[moron]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n",
       "      <td>Such vitriol from the left.  Who would have th...</td>\n",
       "      <td>[hypocrites]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>[]</td>\n",
       "      <td>It is now time for most of you to expand your ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>[828, 829, 830, 831]</td>\n",
       "      <td>Why does this author think she can demand, or ...</td>\n",
       "      <td>[dumb]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0     [84, 85, 86, 87, 88, 89, 90, 91, 133, 134, 135...   \n",
       "1                              [81, 82, 83, 84, 85, 86]   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...   \n",
       "1996                               [23, 24, 25, 26, 27]   \n",
       "1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...   \n",
       "1998                                                 []   \n",
       "1999                               [828, 829, 830, 831]   \n",
       "\n",
       "                                                   text  \\\n",
       "0     That's right. They are not normal. And I am st...   \n",
       "1     \"Watch people die from taking away their healt...   \n",
       "2     tens years ago i contacted the PDR and suggest...   \n",
       "3     The parallels between the ANC and the Sicilian...   \n",
       "4     Intel Community: ‘How can we work for a Presid...   \n",
       "...                                                 ...   \n",
       "1995  hey loser change your name to something more a...   \n",
       "1996  And you are a complete moron who obviously doe...   \n",
       "1997  Such vitriol from the left.  Who would have th...   \n",
       "1998  It is now time for most of you to expand your ...   \n",
       "1999  Why does this author think she can demand, or ...   \n",
       "\n",
       "                           token  \\\n",
       "0     [ABNORMAL, sexist rubbish]   \n",
       "1                       [stupid]   \n",
       "2                             []   \n",
       "3                             []   \n",
       "4                             []   \n",
       "...                          ...   \n",
       "1995    [loser, ignorant, loser]   \n",
       "1996                     [moron]   \n",
       "1997                [hypocrites]   \n",
       "1998                          []   \n",
       "1999                      [dumb]   \n",
       "\n",
       "                                                    seq  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1995  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "1996  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1998         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1613034447824,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "tu3EjmPXbB8A",
    "outputId": "5d4890e3-fb1f-4c29-8dc4-08f32dc16cc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>[49, 50, 51, 52, 53, 54]</td>\n",
       "      <td>Ah, so sad. It certainly does seem to bother t...</td>\n",
       "      <td>[trolls]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7920</th>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...</td>\n",
       "      <td>Yes, let's end the insanity!  No more trans pe...</td>\n",
       "      <td>[No more trans people! No more boys kissing bo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7921</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>Such garbage logic by republicans which will b...</td>\n",
       "      <td>[Such garbage logic by republicans]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...</td>\n",
       "      <td>aa the usual left wing attack of stupidity is ...</td>\n",
       "      <td>[stupidity, crying that wont stop]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7923</th>\n",
       "      <td>[37, 38, 39, 40, 41]</td>\n",
       "      <td>The G&amp;M doesn't need to repeat EVERY idiot thi...</td>\n",
       "      <td>[idiot]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>[177, 178, 179, 180, 181, 182, 183, 184, 185]</td>\n",
       "      <td>All this reality tv suspense, all this loud no...</td>\n",
       "      <td>[stupidity]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>[15, 16, 17, 18, 19, 20]</td>\n",
       "      <td>Can't get over idiots like yourself who probab...</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>[31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...</td>\n",
       "      <td>I only use the word haole when stupidity and a...</td>\n",
       "      <td>[stupidity, arrogance]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>[132, 133, 134, 135]</td>\n",
       "      <td>Pity Vegter doesn't apply the same rigour he n...</td>\n",
       "      <td>[dick]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>[54, 55, 56, 57]</td>\n",
       "      <td>Anyone who voted for Hillary is blind, gullibl...</td>\n",
       "      <td>[fool]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7929</th>\n",
       "      <td>[26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...</td>\n",
       "      <td>No accountability on both sides. No clue on wh...</td>\n",
       "      <td>[sides, idiots]</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>[320, 321, 322, 323, 324, 350, 351, 352, 389, ...</td>\n",
       "      <td>Yes, it is OK. I haven't seen any rioting and ...</td>\n",
       "      <td>[dirty, gay, racist, morons]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>[44, 45, 46, 47, 48, 49, 50, 51]</td>\n",
       "      <td>In other word, blah, blah, blah, blah. It's bu...</td>\n",
       "      <td>[bullshit]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>[218, 219, 220, 221, 222, 223]</td>\n",
       "      <td>Driving like that is why people are going to s...</td>\n",
       "      <td>[idiots]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>[4, 5, 6, 7, 8, 35, 36, 37, 38]</td>\n",
       "      <td>The idiot in chief can't speak for sh!t 😜👎</td>\n",
       "      <td>[idiot, sh!t]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>[8, 9, 10, 11]</td>\n",
       "      <td>Another fool pipes in.</td>\n",
       "      <td>[fool]</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n",
       "      <td>So if a restaurant owner puts up a sign saying...</td>\n",
       "      <td>[No Blacks Allowed]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>Any faith that can't stand up to logic and rea...</td>\n",
       "      <td>[Any faith that can't stand up to logic and re...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
       "      <td>This idiotic. Use the surplus to pay down the ...</td>\n",
       "      <td>[idiotic]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7938</th>\n",
       "      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n",
       "      <td>Who is this \"we\" of which you speak? Are you r...</td>\n",
       "      <td>[penis, women's bodies]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "7919                           [49, 50, 51, 52, 53, 54]   \n",
       "7920  [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 4...   \n",
       "7921  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "7922  [33, 34, 35, 36, 37, 38, 39, 40, 41, 96, 97, 9...   \n",
       "7923                               [37, 38, 39, 40, 41]   \n",
       "7924      [177, 178, 179, 180, 181, 182, 183, 184, 185]   \n",
       "7925                           [15, 16, 17, 18, 19, 20]   \n",
       "7926  [31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 4...   \n",
       "7927                               [132, 133, 134, 135]   \n",
       "7928                                   [54, 55, 56, 57]   \n",
       "7929  [26, 27, 28, 29, 30, 114, 115, 116, 117, 118, ...   \n",
       "7930  [320, 321, 322, 323, 324, 350, 351, 352, 389, ...   \n",
       "7931                   [44, 45, 46, 47, 48, 49, 50, 51]   \n",
       "7932                     [218, 219, 220, 221, 222, 223]   \n",
       "7933                    [4, 5, 6, 7, 8, 35, 36, 37, 38]   \n",
       "7934                                     [8, 9, 10, 11]   \n",
       "7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...   \n",
       "7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "7937                            [5, 6, 7, 8, 9, 10, 11]   \n",
       "7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...   \n",
       "\n",
       "                                                   text  \\\n",
       "7919  Ah, so sad. It certainly does seem to bother t...   \n",
       "7920  Yes, let's end the insanity!  No more trans pe...   \n",
       "7921  Such garbage logic by republicans which will b...   \n",
       "7922  aa the usual left wing attack of stupidity is ...   \n",
       "7923  The G&M doesn't need to repeat EVERY idiot thi...   \n",
       "7924  All this reality tv suspense, all this loud no...   \n",
       "7925  Can't get over idiots like yourself who probab...   \n",
       "7926  I only use the word haole when stupidity and a...   \n",
       "7927  Pity Vegter doesn't apply the same rigour he n...   \n",
       "7928  Anyone who voted for Hillary is blind, gullibl...   \n",
       "7929  No accountability on both sides. No clue on wh...   \n",
       "7930  Yes, it is OK. I haven't seen any rioting and ...   \n",
       "7931  In other word, blah, blah, blah, blah. It's bu...   \n",
       "7932  Driving like that is why people are going to s...   \n",
       "7933         The idiot in chief can't speak for sh!t 😜👎   \n",
       "7934                             Another fool pipes in.   \n",
       "7935  So if a restaurant owner puts up a sign saying...   \n",
       "7936  Any faith that can't stand up to logic and rea...   \n",
       "7937  This idiotic. Use the surplus to pay down the ...   \n",
       "7938  Who is this \"we\" of which you speak? Are you r...   \n",
       "\n",
       "                                                  token  \\\n",
       "7919                                           [trolls]   \n",
       "7920  [No more trans people! No more boys kissing bo...   \n",
       "7921                [Such garbage logic by republicans]   \n",
       "7922                 [stupidity, crying that wont stop]   \n",
       "7923                                            [idiot]   \n",
       "7924                                        [stupidity]   \n",
       "7925                                           [idiots]   \n",
       "7926                             [stupidity, arrogance]   \n",
       "7927                                             [dick]   \n",
       "7928                                             [fool]   \n",
       "7929                                    [sides, idiots]   \n",
       "7930                       [dirty, gay, racist, morons]   \n",
       "7931                                         [bullshit]   \n",
       "7932                                           [idiots]   \n",
       "7933                                      [idiot, sh!t]   \n",
       "7934                                             [fool]   \n",
       "7935                                [No Blacks Allowed]   \n",
       "7936  [Any faith that can't stand up to logic and re...   \n",
       "7937                                          [idiotic]   \n",
       "7938                            [penis, women's bodies]   \n",
       "\n",
       "                                                    seq  \n",
       "7919         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "7920  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7921   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7922  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7923  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "7924  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7925  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7926  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "7927  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7928            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "7929  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7930  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7931  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "7932  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7933               [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  \n",
       "7934                                    [0, 1, 0, 0, 0]  \n",
       "7935  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, ...  \n",
       "7936         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]  \n",
       "7937  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7938  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show example of training data\n",
    "train.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "viHoxDqEauyj"
   },
   "outputs": [],
   "source": [
    "# counting word in spans for train \n",
    "len_span = train['token'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sw7qNdR-VppI"
   },
   "outputs": [],
   "source": [
    "# counting word in spans for test \n",
    "len_span_test = test['token'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1613034465286,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "ZrGstC0whNKs",
    "outputId": "84f32782-36eb-4a21-ef8a-a26a080f8d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0.676534\n",
       "2     0.200655\n",
       "0     0.061091\n",
       "3     0.043330\n",
       "4     0.011588\n",
       "5     0.003401\n",
       "6     0.001512\n",
       "7     0.001008\n",
       "8     0.000504\n",
       "25    0.000126\n",
       "9     0.000126\n",
       "11    0.000126\n",
       "Name: token, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic spans by number of word in span for train \n",
    "len_span.value_counts(normalize=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1613034459712,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "jUR6XwF2VuEz",
    "outputId": "4a01a550-5700-4d4e-bca8-b91d7d65dea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.7035\n",
       "0    0.1970\n",
       "2    0.0860\n",
       "3    0.0080\n",
       "4    0.0040\n",
       "6    0.0010\n",
       "7    0.0005\n",
       "Name: token, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistic spans by number of word in span for test \n",
    "len_span_test.value_counts(normalize=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1613034461050,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "gPJWuIpegKo1",
    "outputId": "6d3b9b0f-1523-42bc-eabc-09d6bf1d4c62"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution histogram plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(len_span, density=True, edgecolor='k', rwidth=0.8)  # density=False would make counts\n",
    "\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Number of span');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq3YWgcPFkB1"
   },
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19480,
     "status": "ok",
     "timestamp": 1613030249939,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "s4txBO5zya-5",
    "outputId": "a3703690-324f-4d99-c80a-0b4171c205b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "# Read embedding\n",
    "word_dict = []\n",
    "embeddings_index = {}\n",
    "f = open('/home/harsh/Downloads/glove.twitter.27B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] \n",
    "    word_dict.append(word)\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "n8bsiH8BSGjC"
   },
   "outputs": [],
   "source": [
    "words = word_dict\n",
    "num_words = len(words)\n",
    "\n",
    "# Dictionary word:index pair\n",
    "# word is key and its value is corresponding index\n",
    "word_to_index = {w : i + 2 for i, w in enumerate(words)}\n",
    "word_to_index[\"UNK\"] = 1\n",
    "word_to_index[\"PAD\"] = 0\n",
    "\n",
    "# Dictionary lable:index pair\n",
    "idx2word = {i: w for w, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3OBCNzNnas7p"
   },
   "outputs": [],
   "source": [
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_to_index.items():\n",
    "    if i > max_feature:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "exfoi9XHba3G"
   },
   "outputs": [],
   "source": [
    " # mapping for token cases\n",
    "case2Idx = {'1': 1, '0': 0}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
    "    char2Idx[c] = len(char2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmJ-NOODFb0Y"
   },
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZpsRfA9QF9mL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['seq']\n",
    "X = data['text']\n",
    "\n",
    "y_test = test['seq']\n",
    "X_test = test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bsS9aVEVcJQO"
   },
   "outputs": [],
   "source": [
    "#train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19143,
     "status": "ok",
     "timestamp": 1613030250911,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "Z8GGEAurVu_c",
    "outputId": "17065691-6cc6-496a-d90e-d8b860869a20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/harsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/harsh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "2022-12-20 19:40:37.983463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 19:40:37.983485: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "tknzr2 = TweetTokenizer()\n",
    "\n",
    "def custom_tokenizer(text_data):\n",
    "    text_data = text_data.lower()\n",
    "    return tknzr2.tokenize(text_data)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    for w in word_list:\n",
    "        w = lemma.lemmatize(w)\n",
    "\n",
    "    new_text = \"\"\n",
    "    for w in word_list:\n",
    "        new_text = new_text + \" \" + w\n",
    "\n",
    "    return new_text\n",
    "\n",
    "def encoding(X, y, isTest = True):\n",
    "    sentences = []\n",
    "    \n",
    "    for t in X:\n",
    "        sentences.append(custom_tokenizer(t))\n",
    "\n",
    "    X = []\n",
    "    for s in sentences:\n",
    "        sent = []\n",
    "        for w in s:\n",
    "            try:\n",
    "                w = w.lower()\n",
    "                sent.append(word_to_index[w])\n",
    "            except:\n",
    "                sent.append(word_to_index[\"UNK\"])\n",
    "        X.append(sent)\n",
    "           \n",
    "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n",
    "\n",
    "    if isTest:\n",
    "        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n",
    "        y = to_categorical(y, num_classes=2)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return (X,y)\n",
    "\n",
    "\n",
    "def decoding(text_data, encoding_text, prediction):\n",
    "    test = [[idx2word[i] for i in row] for row in encoding_text]\n",
    "\n",
    "    lst_token = []\n",
    "\n",
    "    for t in range(0, len(test)):\n",
    "        yy_pred = []\n",
    "        for i in range(0, len(test[t])):\n",
    "            if prediction[t][i] == 1:\n",
    "                yy_pred.append(test[t][i])\n",
    "        lst_token.append(yy_pred)\n",
    "\n",
    "    lis_idx = []\n",
    "    for i in range(0, len(text_data)):\n",
    "        idx = []\n",
    "        for t in lst_token[i]:\n",
    "            index = text_data[i].find(t)\n",
    "            idx.append(index)\n",
    "            for j in range(1, len(t)):\n",
    "                index = index + 1\n",
    "                idx.append(index)\n",
    "        lis_idx.append(idx)\n",
    "\n",
    "    return lis_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9ajE_t3U-PiF"
   },
   "outputs": [],
   "source": [
    "X1, y1 = encoding(X_train, y_train)\n",
    "X2, y2 = encoding(X_dev, y_dev)\n",
    "X3, y3 = encoding(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21310,
     "status": "ok",
     "timestamp": 1613030253088,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "ozUGBMYyIAAu",
    "outputId": "7912ee18-23c8-469a-c207-492183eb67c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'only', 'use', 'the', 'word', 'haole', 'when', 'stupidity', 'and', 'arrogance', 'is', 'involved', 'and', 'not', 'all', 'the', 'time', '.', 'excluding', 'the', 'potus', 'of', 'course', '.']\n",
      "I only use the word haole when stupidity and arrogance is involved and not all the time.  Excluding the POTUS of course.\n",
      "[    12    216    718     15    894 724236     94  17046     28  48680\n",
      "     34   6187     28     80     77     15    137      3  81507     15\n",
      "  34058     41   1605      3      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0]\n",
      "[0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Illustrating the data transforming \n",
    "x_t, y_t = encoding(X, y)\n",
    "\n",
    "print(custom_tokenizer(X[7926]))\n",
    "print(X[7926])\n",
    "print(x_t[7926])\n",
    "\n",
    "print(y[7926])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4XdeIpnE7bC"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7f1f07d9a850>\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.13.1\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9931,
     "status": "ok",
     "timestamp": 1613017772467,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "h_BbOL6QMW2_",
    "outputId": "f79f69ff-1169-4ad8-cf9d-86b16fe75500"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m max_len \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m Input(shape \u001b[38;5;241m=\u001b[39m (max_len,))\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43membeddings_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.1\u001b[39m)(model)\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m Bidirectional(LSTM(units \u001b[38;5;241m=\u001b[39m max_len, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))(model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/legacy/interfaces.py:91\u001b[0m, in \u001b[0;36mgenerate_legacy_interface.<locals>.legacy_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     signature \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     89\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate your `\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m object_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` call to the \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     90\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras 2 API: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m signature, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/layers/embeddings.py:90\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mEmbedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py:132\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[1;32m    131\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     name \u001b[38;5;241m=\u001b[39m _to_snake_case(prefix) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_uid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/backend/tensorflow_backend.py:74\u001b[0m, in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"Get the uid for the default graph.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    A unique identifier for the graph.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _GRAPH_UID_DICTS\n\u001b[0;32m---> 74\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _GRAPH_UID_DICTS:\n\u001b[1;32m     76\u001b[0m     _GRAPH_UID_DICTS[graph] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "# BiLSTM - CRF \n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n",
    "max_len =128\n",
    "input = Input(shape = (max_len,))\n",
    "model = Embedding(input_dim=num_words,output_dim=embedding_dim,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=max_len,\n",
    "                    trainable=True)(input)\n",
    "\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n",
    "crf = CRF(2)  \n",
    "out = crf(model)  # output\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 954503,
     "status": "ok",
     "timestamp": 1611720774064,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "YnDO9T5td0wu",
    "outputId": "5ae495c6-8ec1-4fc7-fd45-b984f822662c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 7766 samples, validate on 863 samples\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7766/7766 [==============================] - 66s 8ms/step - loss: 0.1088 - acc: 0.9736 - val_loss: 0.0888 - val_acc: 0.9778\n",
      "Epoch 2/15\n",
      "7766/7766 [==============================] - 60s 8ms/step - loss: 0.0937 - acc: 0.9736 - val_loss: 0.0825 - val_acc: 0.9790\n",
      "Epoch 3/15\n",
      "7766/7766 [==============================] - 59s 8ms/step - loss: 0.0864 - acc: 0.9736 - val_loss: 0.0783 - val_acc: 0.9797\n",
      "Epoch 4/15\n",
      "7766/7766 [==============================] - 58s 8ms/step - loss: 0.0812 - acc: 0.9736 - val_loss: 0.0746 - val_acc: 0.9797\n",
      "Epoch 5/15\n",
      "7766/7766 [==============================] - 60s 8ms/step - loss: 0.0761 - acc: 0.9736 - val_loss: 0.0734 - val_acc: 0.9794\n",
      "Epoch 6/15\n",
      "7766/7766 [==============================] - 59s 8ms/step - loss: 0.0718 - acc: 0.9736 - val_loss: 0.0698 - val_acc: 0.9798\n",
      "Epoch 7/15\n",
      "7766/7766 [==============================] - 59s 8ms/step - loss: 0.0666 - acc: 0.9736 - val_loss: 0.0690 - val_acc: 0.9798\n",
      "Epoch 8/15\n",
      "7766/7766 [==============================] - 58s 7ms/step - loss: 0.0600 - acc: 0.9736 - val_loss: 0.0708 - val_acc: 0.9789\n",
      "Epoch 9/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0522 - acc: 0.9736 - val_loss: 0.0699 - val_acc: 0.9783\n",
      "Epoch 10/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0485 - acc: 0.9736 - val_loss: 0.0720 - val_acc: 0.9777\n",
      "Epoch 11/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0459 - acc: 0.9736 - val_loss: 0.0717 - val_acc: 0.9776\n",
      "Epoch 12/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0436 - acc: 0.9736 - val_loss: 0.0733 - val_acc: 0.9787\n",
      "Epoch 13/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0417 - acc: 0.9736 - val_loss: 0.0709 - val_acc: 0.9768\n",
      "Epoch 14/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0402 - acc: 0.9736 - val_loss: 0.0752 - val_acc: 0.9748\n",
      "Epoch 15/15\n",
      "7766/7766 [==============================] - 57s 7ms/step - loss: 0.0384 - acc: 0.9736 - val_loss: 0.0738 - val_acc: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba0915d9e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'drive/My Drive/CODE/SemVal/model/model_detection_19.h5',\n",
    "                       verbose = 0,\n",
    "                       mode = 'auto',\n",
    "                       save_best_only = True,\n",
    "                       monitor='val_loss')\n",
    "\n",
    "model.fit(X1, np.array(y1), batch_size=64, epochs=15, validation_data=(X2, y2), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIgpo4AyFAxO"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96487,
     "status": "ok",
     "timestamp": 1613029739065,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "Q4M5Pg396yM8",
    "outputId": "cdecffc2-4579-4ca3-8f84-515996a71016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load bi-LSTM model\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "# load model\n",
    "model = load_model('drive/My Drive/CODE/SemVal/model/model_detection_4.h5', custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_accuracy':crf_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K_jXnF2nVVV"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X3)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_test_true = np.argmax(y3, -1)\n",
    "# y_pred = [[i for i in row] for row in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RQpdMVR17DL"
   },
   "outputs": [],
   "source": [
    "# return back\n",
    "test = [[idx2word[i] for i in row] for row in X3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13049,
     "status": "ok",
     "timestamp": 1613029870446,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "MyoE9-1e6pbu",
    "outputId": "13e127a1-a9b7-4767-8329-de2973864894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sexist', 'rubbish']\n",
      "['abnormal', 'sexist', 'rubbish']\n"
     ]
    }
   ],
   "source": [
    "yy_pred = []\n",
    "yy_test = []\n",
    "\n",
    "for i in range(0, len(test[0])):\n",
    "    if y_pred[0][i] == 1:\n",
    "        yy_pred.append(test[0][i])\n",
    "\n",
    "for i in range(0, len(test[0])):\n",
    "    if y_test_true[0][i] == 1:\n",
    "        yy_test.append(test[0][i])\n",
    "\n",
    "print(yy_pred)\n",
    "print(yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDUPWplLuZZw"
   },
   "outputs": [],
   "source": [
    "raw_y = decoding(X_test, X3, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9974,
     "status": "ok",
     "timestamp": 1613029870859,
     "user": {
      "displayName": "Sơn Lưu Thanh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhQwxcIgL3UNqhClZgF7yUwHHafv0lEQ0dZRDTrbA=s64",
      "userId": "09824077883060402796"
     },
     "user_tz": -420
    },
    "id": "0BKFHGePtTwt",
    "outputId": "59145e5a-5e67-4052-8ab7-df2f31eac2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.320465302853854\n"
     ]
    }
   ],
   "source": [
    "f1(raw_y[0], spans_test[0])\n",
    "\n",
    "acc = []\n",
    "for i in range(0, len(spans_test)):\n",
    "    acc.append(f1(raw_y[i], spans_test[i]))\n",
    "\n",
    "print(np.mean(acc)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07LODzVr3Ntk"
   },
   "outputs": [],
   "source": [
    "# Make CSV file for itegrate BERT \n",
    "new_sub = pd.DataFrame({'id': test_id, 'text': text_data_test, 'spans': raw_y, 'span_true': spans_test})\n",
    "\n",
    "new_sub.to_csv('drive/My Drive/CODE/SemVal/test_demo_detection.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLzznY_nwpac"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scubUW_tkDHX"
   },
   "outputs": [],
   "source": [
    "# Load bi-LSTM model\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "# load model\n",
    "model = load_model('drive/My Drive/CODE/SemVal/model/model_detection_4.h5', custom_objects={'CRF':CRF,'crf_loss':crf_loss,'crf_accuracy':crf_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66brisWy5Y2i"
   },
   "outputs": [],
   "source": [
    "SUBMIUSSION = 'drive/My Drive/CODE/SemVal/dataset/tsd_test.csv'\n",
    "\n",
    "subm = pd.read_csv(SUBMIUSSION)\n",
    "subm_text = subm['text']\n",
    "subm_id = subm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XO7WRwi7c38"
   },
   "outputs": [],
   "source": [
    "X4, y4 = encoding(subm_text, None, isTest=False)\n",
    "\n",
    "y_sub = model.predict(X4)\n",
    "y_sub = np.argmax(y_sub, axis=-1)\n",
    "\n",
    "raw_subm = decoding(subm_text, X4, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ6RpjxDuP5M"
   },
   "outputs": [],
   "source": [
    "# Make CSV file \n",
    "new_sub = pd.DataFrame({'id': subm_id, 'text': subm_text, 'spans': raw_subm})\n",
    "\n",
    "new_sub.to_csv('drive/My Drive/CODE/SemVal/submission_detection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv_MbxGOnIQl"
   },
   "outputs": [],
   "source": [
    "# make sure that the ids match the ones of the scores\n",
    "predictions = raw_subm\n",
    "ids = subm_id\n",
    "\n",
    "# write in a prediction file named \"spans-pred.txt\"\n",
    "with open(\"spans-pred.txt\", \"w\") as out:\n",
    "  for uid, text_scores in zip(ids, predictions):\n",
    "    out.write(f\"{str(uid)}\\t{str(text_scores)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMbkiMiMHEZVe9S36+zq17r",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1R-YJ3AXhSSJbaeV6IU1GsuWjRnpY4yd3",
   "name": "Toxic_Span_Detection_BiLSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
